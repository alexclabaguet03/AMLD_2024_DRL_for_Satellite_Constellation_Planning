{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will create a basic \"Grid World\" environment that mimics the functionality of OpenAI Gym environments. This simplified environment will help you understand the dynamics and challenges of reinforcement learning (RL) by implementing the environment from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Attributes**\n",
    "- **size**: The size of the grid (default 4x4).\n",
    "- **start_position**: The starting position of the agent (bottom-left corner - (0,0)).\n",
    "- **goal_position**: The goal position the agent aims to reach (top-right corner - (size-1, size-1)).\n",
    "- **state**: The current position of the agent on the grid.\n",
    "- **max_episode_length**: The maximum number of steps the agent can take in an episode.\n",
    "- **current_step**: The current step number in the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Existing Methods**\n",
    "\n",
    "- __Innit__: initializes the environment with the given size and maximum episode length.\n",
    "  - Arguments:\n",
    "    - size: the size of the grid.\n",
    "    - max_episode_length: the maximum number of steps the agent can take in an episode.\n",
    "\n",
    "- **Render**: displays the current state of the environment with agent and goal position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods to Implement**\n",
    "\n",
    "- **Reset**: Reset the environment to the initial state.  \n",
    "  - Returns: The initial state of the environment.\n",
    "\n",
    "- **Step**: Take an action in the environment.  \n",
    "  - Arguments:  \n",
    "    - action: The action to take in the environment.\n",
    "  - Features:\n",
    "    - if the state is equal to the goal position, reward is equal to 10\n",
    "    - else the reward is equal to -1\n",
    "    - if the number of steps is superior to the episode lenght, reward is equal to -10\n",
    "  - Returns:  \n",
    "    - state: The new state of the environment.\n",
    "    - reward: The reward for the action taken.\n",
    "    - done: A boolean indicating if the episode has ended.\n",
    "    - info: Additional information.\n",
    "- **Sample Action**: Samples a random action from the action space.  \n",
    "  - Returns: A random action.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleGridWorld:\n",
    "    def __init__(self, size=4, max_episode_length=10):\n",
    "        self.size = size\n",
    "        self.start_position = (0, 0)\n",
    "        self.goal_position = (size - 1, size - 1)\n",
    "        self.state = self.start_position\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.start_position\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    "    \n",
    "    def render(self):\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if (i, j) == self.state:\n",
    "                    print(\"A \", end=\"\")\n",
    "                elif (i, j) == self.goal_position:\n",
    "                    print(\"G \", end=\"\")\n",
    "                else:\n",
    "                    print(\". \", end=\"\")\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.current_step >= self.max_episode_length:\n",
    "            return self.state, -10, True, {}\n",
    "        \n",
    "        self.current_step += 1\n",
    "        x, y = self.state\n",
    "        if action == \"up\" and x > 0:\n",
    "            self.state = (x - 1, y)\n",
    "        elif action == \"down\" and x < self.size - 1:\n",
    "            self.state = (x + 1, y)\n",
    "        elif action == \"left\" and y > 0:\n",
    "            self.state = (x, y - 1)\n",
    "        elif action == \"right\" and y < self.size - 1:\n",
    "            self.state = (x, y + 1)\n",
    "        \n",
    "        if self.state == self.goal_position:\n",
    "            reward = 10\n",
    "        elif self.current_step >= self.max_episode_length:\n",
    "            reward = -10\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        done = self.state == self.goal_position or self.current_step >= self.max_episode_length\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def sample_action(self):\n",
    "        return [\"up\", \"down\", \"left\", \"right\"][np.random.randint(4)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode and Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An episode in RL is a sequence of steps that starts from the initial state and ends when the agent reaches the goal state or the maximum episode length is reached. In each step, the agent takes an action in the environment, receives a reward, and transitions to a new state. The agent continues this process until the episode ends.\n",
    "\n",
    "Here, we instantiate an environment as defined above and sample a trajectory by taking random actions at each step. We will visualize the agent's movement in the grid world and observe the rewards received during the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleGridWorld(size=4, max_episode_length=10)\n",
    "state = env.reset()\n",
    "done = False\n",
    "trajectory = []  # Initialize an empty list to store the trajectory\n",
    "\n",
    "while not done:\n",
    "    action = env.sample_action()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Save the step information in a dictionary format\n",
    "    step_data = {\n",
    "        \"state\": state,\n",
    "        \"action\": action,\n",
    "        \"reward\": reward,\n",
    "        \"next_state\": next_state,\n",
    "        \"done\": done\n",
    "    }\n",
    "    trajectory.append(step_data)  # Append the step data to the trajectory list\n",
    "    \n",
    "    print(f\"Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "    env.render()\n",
    "    \n",
    "    state = next_state  # Update the current state to the next state\n",
    "    \n",
    "    if done:\n",
    "        print(\"Final Objective Attained or Max episode length reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampled trajectory was stored in a dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
